{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Haystack RAG para few-shot con GPT y Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recomendado python 3.10\n",
    "#%pip install haystack-ai==2.2.1 trafilatura==1.10.0 qdrant-haystack==3.8.0\n",
    "#%pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "#%pip install ollama-haystack==0.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requiere:\n",
    "- Contenedor con qdrant (bd vectorial) y reportes en un archivo con la ruta data\\LosCarrera_labeled\\etiquetado_1-456_v1.01\\train.jsonl\n",
    "- Contenedor con langfuse y otro con postgres (se usa para tracing de las consultas)\n",
    "- Archivo .env con:\n",
    "    - OPENAI_API_KEY - asociada a una cuenta que tenga saldo en openai\n",
    "    - LANGFUSE_HOST - url y puerto (si es contenedor docker, puede ser http://localhost:3000)\n",
    "    - LANGFUSE_SECRET_KEY - key generada por langfuse\n",
    "    - LANGFUSE_PUBLIC_KEY - key generada por langfuse\n",
    "    - HAYSTACK_CONTENT_TRACING_ENABLED = True - requerido para habilitar el tracing\n",
    "- Archivo con reportes en la ruta data\\LosCarrera_labeled\\etiquetado_1-456_v1.01\\test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.dataclasses import Document\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.connectors.langfuse import LangfuseConnector\n",
    "\n",
    "# from haystack.components.converters import TextFileToDocument\n",
    "# from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "\n",
    "# from haystack.components.evaluators import DocumentRecallEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECCIONAR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECCIONAR gpt-4o, gpt-4o-mini o llama3.1\n",
    "CHAT_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado dataset \n",
    "strings -> documentos\n",
    "\n",
    "Se requiere convertir los datos del jsonl (reportes etiquetados) a un formato compatible con Haystack (clase Document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer dataset y convertir conjuntamente en un txt y en una lista de Documents\n",
    "\n",
    "data_path = \"data/LosCarrera_labeled/etiquetado_1-456_v1.01/\"\n",
    "text_file_path = data_path + \"train_text.txt\"\n",
    "jsonl_file_path = data_path + \"train.jsonl\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "with open(text_file_path, \"w\") as text_file:\n",
    "    for i, row in pd.read_json(jsonl_file_path, lines=True).iterrows():\n",
    "        text_file.write(\"Reporte: \"+ row[\"text\"] + \"\\n\")\n",
    "        # eliminar entidades de tipo \"GANGLIOS\"\n",
    "        entities = [entity for entity in row[\"entities\"] if entity[\"label\"] != \"GANGLIOS\"]    \n",
    "        text_file.write(\"Entidades: \" + str(entities) + \"\\n\")\n",
    "        text_file.write(\"Relaciones: \" + str(row[\"relations\"]) + \"\\n\\n\\n\")\n",
    "        documents.append(Document(content=row[\"text\"], meta={\"id\": row[\"id\"], \"entities\": entities, \"relations\": row[\"relations\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver un reporte random extraído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_num = random.randint(0, len(documents)-1)\n",
    "\n",
    "print(\"DOCUMENTO:\\n\")\n",
    "print(documents[random_num])\n",
    "\n",
    "print(\"\\n\\nCONTENIDO:\\n\")\n",
    "print(documents[random_num].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding pipeline (qdrant y OpenAI)\n",
    "\n",
    "documentos -> vectores\n",
    "\n",
    "Se crea un pipeline que permite subir los datos a Qdrant. Actualmente, se puede montar qdrant en un docker, asegurándose que use el puerto configurado abajo.\n",
    "\n",
    "El pipeline usa el modelo de OpenAI para generar los vectores (text-embedding-3-large), usando la librería OpenAIDocumentEmbedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setear y correr pipeline de indexado en la BD\n",
    "\n",
    "document_store = QdrantDocumentStore(url=\"http://localhost\",\n",
    "                                     port=6333,\n",
    "                                     embedding_dim=3072,\n",
    "                                     index=\"AIMA_LosCarrera_V1.01\")\n",
    "\n",
    "if document_store.count_documents() == 0:\n",
    "    embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-large\")\n",
    "    writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "    indexing_pipeline = Pipeline()\n",
    "    indexing_pipeline.add_component(\"tracer\", LangfuseConnector(\"Qdrant Document Embedder\"))\n",
    "    indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "    indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "    result = indexing_pipeline.run(data={\"documents\":documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.count_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "\n",
    "Embeddings - OpenAI (se usa para convertir el nuevo reporte a vector y hacer RAG)<br>\n",
    "Chat - OpenAI o LLama (genera el etiquetado una vez hecho el RAG)<br>\n",
    "BD vectorial - Qdrant<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setear pipeline y prompt\n",
    "# from haystack.components.validators import JsonSchemaValidator\n",
    "\n",
    "NUM_EXAMPLES = 0\n",
    "\n",
    "text_embedder = OpenAITextEmbedder(model=\"text-embedding-3-large\")\n",
    "retriever = QdrantEmbeddingRetriever(document_store ,top_k=NUM_EXAMPLES)\n",
    "if CHAT_MODEL == \"gpt-4o-mini\" or CHAT_MODEL == \"gpt-4o\":\n",
    "    if NUM_EXAMPLES > 0:\n",
    "        template = open(data_path + \"NER/prompt_gpt.txt\", \"r\").read()\n",
    "    elif NUM_EXAMPLES == 0:\n",
    "        template = open(data_path + \"NER/prompt_gpt_zero-shot.txt\", \"r\").read()\n",
    "        retriever = QdrantEmbeddingRetriever(document_store ,top_k=1)\n",
    "    llm = OpenAIGenerator(model=CHAT_MODEL)\n",
    "elif CHAT_MODEL == \"llama3.1\":\n",
    "    template = open(data_path + \"NER/prompt_ollama.txt\", \"r\").read()\n",
    "    llm = OllamaGenerator(model=CHAT_MODEL, url=\"http://localhost:11434/api/generate\")\n",
    "else:\n",
    "    raise ValueError(\"CHAT_MODEL debe ser 'gpt-4o', 'gpt-4o-mini' o 'llama3.1'\")\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "rag_pipeline.add_component(\"tracer\", LangfuseConnector(\"Mammography Few-Shot RAG NER \"+ CHAT_MODEL))\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "# rag_pipeline.add_component(\"schema_validator\", JsonSchemaValidator())\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "# rag_pipeline.connect(\"llm.response\", \"schema_validator.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts a la API del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir dataset a enviar mediante prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_file_path = data_path + \"test.jsonl\"\n",
    "\n",
    "# load validation data\n",
    "\n",
    "prompt_reports = pd.read_json(reports_file_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop de envío"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "# Setear número de reportes a enviar a la API, es un prompt por reporte\n",
    "NUM_REPORTS = 69 # 69 es el total del conjunto de test\n",
    "\n",
    "# \n",
    "processed_ids_path = \"processed_ids.txt\"\n",
    "if os.path.exists(processed_ids_path):\n",
    "    with open(processed_ids_path, \"r\") as f:\n",
    "        processed_ids_list = f.read().splitlines()\n",
    "else:\n",
    "    processed_ids_list = []\n",
    "\n",
    "for i, informe in enumerate(prompt_reports[\"text\"][:NUM_REPORTS]):\n",
    "    query = informe\n",
    "    report_id = prompt_reports[\"id\"][i]\n",
    "    # si el id del reporte ya está en la lista de ids procesados, no se envía a la API\n",
    "    if report_id in processed_ids_list:\n",
    "        print(\"Reporte \", i+1, \" de \", NUM_REPORTS, \" ya procesado\")\n",
    "        continue\n",
    "    result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "    # if the string contains ```json and ``` remove them\n",
    "    result_str = result[\"llm\"][\"replies\"][0]\n",
    "    result_str = result_str.replace(\"```json\", \"\")\n",
    "    result_str = result_str.replace(\"```\", \"\")\n",
    "    # convert result from string to a list of dictionaries\n",
    "    try:\n",
    "        result_json = eval(result_str)\n",
    "        result_json = {\"id\": report_id, \"entities\": result_json}\n",
    "        responses.append(result_json)\n",
    "        print(\"Procesado reporte \", i+1, \" de \", NUM_REPORTS)\n",
    "        processed_ids_list.append(report_id)\n",
    "    except:\n",
    "        print(\"Error en el reporte \", i+1, \" de \", NUM_REPORTS)\n",
    "    print(\"id: \", report_id)\n",
    "    # print(informe)\n",
    "    # print(json.dumps(result_json, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Guardar el listado de ids procesados en un archivo txt en la carpeta actual\n",
    "with open(processed_ids_path, \"w\") as f:\n",
    "    for item in processed_ids_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "print(\"Número de reportes enviados a la API: \", len(responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arreglar Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Levenshtein\n",
    "\n",
    "\"\"\"\n",
    "    fix_spans\n",
    "    Explicacion : función que recibe un conjunto de entidades (resultado de un NER) y el texto asociado, y corrige los spans de las entidades para que coincidan con las posiciones de las palabras en el texto. La manera de hacerlo es buscar el texto de la entidad en el texto asociado, y encontrar la posición de la primera y última palabra de la entidad en el texto. Si hay más de una coincidencia, se elige la que esté más cerca de la posición original de la entidad. Si no hay coincidencias, se busca la primera palabra del span de la entidad en el texto, y se busca la coincidencia más cercana a la posición original de la entidad, luego, se compara la distancia de Levenshtein entre el texto de la entidad y el texto original comenzando desde la posición encontrada (con un largo igual al largo del span de la entidad), si la distancia es menor a un umbral, se considera que se encontró la posición correcta y se ajusta el span de la entidad.\n",
    "\n",
    "    Input : \n",
    "    - report_entities : lista de diccionarios, donde cada diccionario tiene 4 llaves: \"label\", \"start_offset\", \"end_offset\" y \"span_text\".\n",
    "    - full_text : string, texto completo del que se extrajeron las entidades.\n",
    "\n",
    "    Output :\n",
    "    - fixed_entities : lista de diccionarios, donde cada diccionario tiene 4 llaves: \"label\", \"start_offset\", \"end_offset\" y \"span_text\", con los spans corregidos.\n",
    "\"\"\"\n",
    "\n",
    "def fix_spans(report_entities, full_text):\n",
    "    span_difference_threshold = 5\n",
    "    fixed_entities = []\n",
    "\n",
    "    for i, entity in enumerate(report_entities):\n",
    "    \n",
    "        span_text = entity[\"span_text\"]\n",
    "        start_pos = entity[\"start_offset\"]\n",
    "        end_pos = entity[\"end_offset\"]        \n",
    "\n",
    "        # Buscar coincidencia exacta del texto de la entidad en el texto completo\n",
    "        if span_text in full_text:\n",
    "            # si hay más de una coincidencia, elegir la que esté más cerca de la posición original de la entidad\n",
    "            # Usar escape para que no haya problemas con caracteres especiales\n",
    "            escaped_span_text = re.escape(span_text)\n",
    "            start_positions = [m.start() for m in re.finditer(escaped_span_text, full_text)]\n",
    "            if len(start_positions) > 1:\n",
    "                distances = [abs(start_pos - pos) for pos in start_positions]\n",
    "                start_pos = start_positions[distances.index(min(distances))]\n",
    "            else:\n",
    "                start_pos = start_positions[0]\n",
    "            end_pos = start_pos + len(span_text)\n",
    "            fixed_entities.append({\"label\": entity[\"label\"], \"start_offset\": start_pos, \"end_offset\": end_pos, \"span_text\": span_text})\n",
    "            print(\"Corregido mediante coincidencia exacta\")\n",
    "            print(\"Start positions: \", start_positions)\n",
    "            print(\"Entidad: \", span_text)\n",
    "        else:\n",
    "            # Extraer la primera palabra del span de la entidad\n",
    "            first_word = span_text.split()[0]\n",
    "            # Buscar la primera palabra en el texto completo\n",
    "            if first_word in full_text:\n",
    "                # si hay más de una coincidencia, elegir la que esté más cerca de la posición original de la entidad\n",
    "                # Usar escape para que no haya problemas con caracteres especiales\n",
    "                escaped_first_word = re.escape(first_word)\n",
    "                start_positions = [m.start() for m in re.finditer(escaped_first_word, full_text)]\n",
    "                if len(start_positions) > 1:\n",
    "                    distances = [abs(start_pos - pos) for pos in start_positions]\n",
    "                    start_pos = start_positions[distances.index(min(distances))]\n",
    "                end_pos = start_pos + len(span_text)\n",
    "\n",
    "                if end_pos > len(full_text):\n",
    "                    end_pos = len(full_text)\n",
    "\n",
    "                # Comparar la distancia de Levenshtein entre el texto de la entidad y el texto original comenzando desde la posición encontrada\n",
    "                # si la distancia es menor a un umbral, se considera que se encontró la posición correcta\n",
    "                if Levenshtein.distance(full_text[start_pos:end_pos], span_text) < span_difference_threshold:\n",
    "                    fixed_entities.append({\"label\": entity[\"label\"], \"start_offset\": start_pos, \"end_offset\": end_pos, \"span_text\": span_text})\n",
    "                    print(\"Corregido mediante Levenshtein\")\n",
    "                else:\n",
    "                    print(\"No se pudo corregir\", span_text, start_pos, end_pos)\n",
    "            else:\n",
    "                print(\"No se pudo corregir\", span_text, start_pos, end_pos)\n",
    "    return fixed_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arreglar spans y guardar respuestas en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fixed_responses = []\n",
    "\n",
    "for i, response_object in enumerate(responses):\n",
    "    print(\"\\n\\nInforme: \", i)\n",
    "    response_entities = response_object[\"entities\"]  \n",
    "    fixed_entities = fix_spans(response_entities, prompt_reports[\"text\"][i])\n",
    "    fixed_responses.append({\"id\": response_object[\"id\"], \"entities\": fixed_entities})\n",
    "# print(json.dumps(fixed_responses, indent=4, ensure_ascii=False))\n",
    "\n",
    "# guardar resultados en un archivo jsonl en la carpeta actual, append mode\n",
    "output_file_path = \"output.jsonl\"\n",
    "\n",
    "with open(output_file_path, \"a\") as f:\n",
    "    for item in fixed_responses:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_spans_iou(span1, span2):\n",
    "    start1, end1 = span1\n",
    "    start2, end2 = span2\n",
    "    intersection = max(0, min(end1, end2) - max(start1, start2))\n",
    "    union = min(max(end1, end2) - min(start1, start2), end1-start1 + end2-start2)\n",
    "    return intersection/union\n",
    "\n",
    "def compare_entities(entity1, entity2):\n",
    "    start_offset_distance_threshold = 20\n",
    "    iou_threshold = 0.5\n",
    "    entities_are_equal = False\n",
    "    # si las entidades no tienen el mismo label, no son iguales\n",
    "    if entity1[\"label\"] != entity2[\"label\"]:\n",
    "        return entities_are_equal\n",
    "    # si el iou entre los spans de las entidades es menor al umbral definido, no son iguales\n",
    "    if compare_spans_iou([entity1[\"start_offset\"], entity1[\"end_offset\"]], [entity2[\"start_offset\"], entity2[\"end_offset\"]]) < iou_threshold:\n",
    "        return entities_are_equal\n",
    "    else:\n",
    "        # si el iou es mayor al umbral, chequear si el comienzo de ambas está a menos de una distancia umbral\n",
    "        if abs(entity1[\"start_offset\"] - entity2[\"start_offset\"]) > start_offset_distance_threshold:\n",
    "            return entities_are_equal\n",
    "\n",
    "    entities_are_equal = True\n",
    "    return entities_are_equal\n",
    "\n",
    "# contar tp, fp, fn para cada clase para un reporte\n",
    "def calculate_tp_fp_fn_report(gold_entities, generated_entities, classes):\n",
    "    num_classes = len(classes)\n",
    "    tp = [0]*num_classes\n",
    "    fp = [0]*num_classes\n",
    "    fn = [0]*num_classes\n",
    "\n",
    "    for gold_entity in gold_entities:\n",
    "        found = False\n",
    "        for generated_entity in generated_entities:\n",
    "            if compare_entities(gold_entity, generated_entity):\n",
    "                tp[classes.index(gold_entity[\"label\"])] += 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            fn[classes.index(gold_entity[\"label\"])] += 1\n",
    "\n",
    "    for generated_entity in generated_entities:\n",
    "        found = False\n",
    "        for gold_entity in gold_entities:\n",
    "            if compare_entities(gold_entity, generated_entity):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            if generated_entity[\"label\"] in classes:\n",
    "                fp[classes.index(generated_entity[\"label\"])] += 1\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "# calcular precision, recall y f1 dados tp, fp y fn\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    precision = tp/(tp+fp) if tp+fp > 0 else 0\n",
    "    recall = tp/(tp+fn) if tp+fn > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if precision+recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejecución (cálculo de métricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer entidades corregidas de los informes desde el archivo jsonl output.jsonl\n",
    "fixed_responses = []\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        fixed_responses.append(json.loads(line))\n",
    "\n",
    "print(\"Número de informes con entidades corregidas: \", len(fixed_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or open csv file to save results\n",
    "results_file_path = data_path + \"NER/results.csv\"\n",
    "if not os.path.exists(results_file_path):\n",
    "    results_df = pd.DataFrame(columns=[\"chat_model\",\"fecha/hora\", \"num_reports\", \"macro_f1\", \"micro_f1\", \"HALL_presente_f1\", \"HALL_ausente_f1\", \"CARACT_f1\", \"CUAD_f1\", \"LAT_f1\", \"REG_f1\", \"DENS_f1\"])\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "else:\n",
    "    results_df = pd.read_csv(results_file_path)\n",
    "\n",
    "# calcular tp, fp, fn para cada clase\n",
    "classes = [\"HALL_presente\", \"HALL_ausente\", \"CARACT\", \"CUAD\", \"LAT\", \"REG\", \"DENS\"]\n",
    "gold_entities_counts = {}\n",
    "tp_total = [0]*len(classes)\n",
    "fp_total = [0]*len(classes)\n",
    "fn_total = [0]*len(classes)\n",
    "\n",
    "print(\"Scores por clase\")\n",
    "        \n",
    "# por cada informe, llamar a la función calculate_tp_fp_fn_report. Sumar los resultados de cada informe para calcular tp, fp, fn totales.\n",
    "for i, informe in enumerate(prompt_reports[\"text\"][:NUM_REPORTS]):\n",
    "    gold_entities = prompt_reports[\"entities\"][i]\n",
    "    # eliminar entidades de tipo \"GANGLIOS\"\n",
    "    gold_entities = [entity for entity in gold_entities if entity[\"label\"] != \"GANGLIOS\"]\n",
    "    # contar cantidad de entidades de cada clase\n",
    "    for entity in gold_entities:\n",
    "        if entity[\"label\"] in gold_entities_counts:\n",
    "            gold_entities_counts[entity[\"label\"]] += 1\n",
    "        else:\n",
    "            gold_entities_counts[entity[\"label\"]] = 1\n",
    "\n",
    "    # buscar entidades corregidas del informe, buscando por id en la lista fixed_responses\n",
    "    generated_entities = []\n",
    "    for response in fixed_responses:\n",
    "        if response[\"id\"] == prompt_reports[\"id\"][i]:\n",
    "            generated_entities = response[\"entities\"]\n",
    "    if len(generated_entities) == 0:\n",
    "        print(\"No se encontraron entidades corregidas para el informe \", i)\n",
    "        continue\n",
    "    tp, fp, fn = calculate_tp_fp_fn_report(gold_entities, generated_entities, classes)\n",
    "    tp_total = [sum(x) for x in zip(tp_total, tp)]\n",
    "    fp_total = [sum(x) for x in zip(fp_total, fp)]\n",
    "    fn_total = [sum(x) for x in zip(fn_total, fn)]\n",
    "\n",
    "# calcular precision, recall y f1 para cada clase\n",
    "metrics_per_class = []\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(\"Clase: \", class_name)\n",
    "    metrics = calculate_metrics(tp_total[i], fp_total[i], fn_total[i])\n",
    "    print(\"Precision: \", metrics[0])\n",
    "    print(\"Recall: \", metrics[1])\n",
    "    print(\"F1: \", metrics[2])\n",
    "    metrics_per_class.append(metrics)\n",
    "    # mostrar cantidad de entidades de la clase\n",
    "    if class_name in gold_entities_counts:\n",
    "        print(\"Cantidad de entidades de la clase: \", gold_entities_counts[class_name])\n",
    "\n",
    "# macro-average\n",
    "macro_precision = 0\n",
    "macro_recall = 0\n",
    "macro_f1 = 0\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    macro_precision += metrics_per_class[i][0]/len(classes)\n",
    "    macro_recall += metrics_per_class[i][1]/len(classes)\n",
    "    macro_f1 += metrics_per_class[i][2]/len(classes)\n",
    "\n",
    "print(\"\\nPromedios\")\n",
    "\n",
    "\n",
    "print(\"Macro-average\")\n",
    "print(\"Precision: \", macro_precision)\n",
    "print(\"Recall: \", macro_recall)\n",
    "print(\"F1: \", macro_f1)\n",
    "\n",
    "# micro-average\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = calculate_metrics(sum(tp_total), sum(fp_total), sum(fn_total))\n",
    "\n",
    "print(\"Micro-average\")\n",
    "print(\"Precision: \", micro_precision)\n",
    "print(\"Recall: \", micro_recall)\n",
    "print(\"F1: \", micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar resultados en csv\n",
    "import datetime\n",
    "results_df = pd.concat([results_df, pd.DataFrame([[CHAT_MODEL,datetime.datetime.now(), NUM_REPORTS, macro_f1, micro_f1, metrics_per_class[0][2], metrics_per_class[1][2], metrics_per_class[2][2], metrics_per_class[3][2], metrics_per_class[4][2], metrics_per_class[5][2], metrics_per_class[6][2]]], columns=results_df.columns)], ignore_index=True)\n",
    "\n",
    "\n",
    "results_df.to_csv(results_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
