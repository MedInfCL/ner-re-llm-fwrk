{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Haystack RAG para few-shot con GPT y Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recomendado python 3.10\n",
    "#%pip install haystack-ai==2.2.1 trafilatura==1.10.0 qdrant-haystack==3.8.0\n",
    "#%pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "#%pip install ollama-haystack==0.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requiere:\n",
    "- Contenedor con qdrant (bd vectorial) y reportes en un archivo con la ruta data\\LosCarrera_labeled\\etiquetado_1-456_v1.01\\train.jsonl\n",
    "- Contenedor con langfuse y otro con postgres (se usa para tracing de las consultas)\n",
    "- Archivo .env con:\n",
    "    - OPENAI_API_KEY - asociada a una cuenta que tenga saldo en openai\n",
    "    - LANGFUSE_HOST - url y puerto (si es contenedor docker, puede ser http://localhost:3000)\n",
    "    - LANGFUSE_SECRET_KEY - key generada por langfuse\n",
    "    - LANGFUSE_PUBLIC_KEY - key generada por langfuse\n",
    "    - HAYSTACK_CONTENT_TRACING_ENABLED = True - requerido para habilitar el tracing\n",
    "- Archivo con reportes en la ruta data\\LosCarrera_labeled\\etiquetado_1-456_v1.01\\test.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from haystack import Pipeline\n",
    "from haystack.dataclasses import Document\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "from haystack_integrations.components.connectors.langfuse import LangfuseConnector\n",
    "\n",
    "# from haystack.components.converters import TextFileToDocument\n",
    "# from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "\n",
    "# from haystack.components.evaluators import DocumentRecallEvaluator\n",
    "\n",
    "import pandas as pd\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECCIONAR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECCIONAR gpt-4o, gpt-4o-mini o llama3.1\n",
    "CHAT_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para convertir la numeración de los ids en números partiendo desde el 0\n",
    "\n",
    "(Esto es para que sea más fácil para el modelo asociar ids)\n",
    "\n",
    "Esta función se utiliza tanto al guardar los reportes en la bd vectorial, como al enviar el prompt y evaluar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids(entities, relations):\n",
    "    # convertir los ids de las entidades y relaciones de manera que comiencen desde 0 en las entidades, y se mantenga la correlación con las relaciones\n",
    "    entities_dict = {entity[\"id\"]: i for i, entity in enumerate(entities)}\n",
    "    relations = [{\"from_id\": entities_dict[rel[\"from_id\"]], \"to_id\": entities_dict[rel[\"to_id\"]], \"type\": rel[\"type\"]} for rel in relations]\n",
    "    entities = [{\"id\": entities_dict[entity[\"id\"]], \"label\": entity[\"label\"], \"start_offset\": entity[\"start_offset\"], \"end_offset\": entity[\"end_offset\"]} for entity in entities]\n",
    "    return entities, relations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesado dataset \n",
    "strings -> documentos\n",
    "\n",
    "Se requiere convertir los datos del jsonl (reportes etiquetados) a un formato compatible con Haystack (clase Document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer dataset y convertir conjuntamente en un txt y en una lista de Documents\n",
    "\n",
    "data_path = \"data/LosCarrera_labeled/etiquetado_1-456_v1.01/\"\n",
    "jsonl_file_path = data_path + \"test.jsonl\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i, row in pd.read_json(jsonl_file_path, lines=True).iterrows():\n",
    "    # eliminar entidades de tipo \"GANGLIOS\"\n",
    "    entities = [entity for entity in row[\"entities\"] if entity[\"label\"] != \"GANGLIOS\"]\n",
    "    entities, relations = convert_ids(entities, row[\"relations\"]) \n",
    "    documents.append(Document(content=row[\"text\"], meta={\"id\": row[\"id\"], \"entities\": entities, \"relations\": relations}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver un reporte random extraído"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_num = random.randint(0, len(documents)-1)\n",
    "\n",
    "print(\"DOCUMENTO:\\n\")\n",
    "print(documents[random_num])\n",
    "\n",
    "print(\"\\n\\nCONTENIDO:\\n\")\n",
    "print(documents[random_num].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding pipeline (qdrant y OpenAI)\n",
    "\n",
    "documentos -> vectores\n",
    "\n",
    "Se crea un pipeline que permite subir los datos a Qdrant. Actualmente, se puede montar qdrant en un docker, asegurándose que use el puerto configurado abajo.\n",
    "\n",
    "El pipeline usa el modelo de OpenAI para generar los vectores (text-embedding-3-large), usando la librería OpenAIDocumentEmbedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setear y correr pipeline de indexado en la BD\n",
    "\n",
    "document_store = QdrantDocumentStore(url=\"http://localhost\",\n",
    "                                     port=6333,\n",
    "                                     embedding_dim=3072,\n",
    "                                     index=\"AIMA_LosCarrera_RE_v1.01\",)\n",
    "\n",
    "if document_store.count_documents() == 0:\n",
    "    embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-large\")\n",
    "    writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "    indexing_pipeline = Pipeline()\n",
    "    indexing_pipeline.add_component(\"tracer\", LangfuseConnector(\"Qdrant Document Embedder\"))\n",
    "    indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "    indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "    indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "    result = indexing_pipeline.run(data={\"documents\":documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.count_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "\n",
    "Embeddings - OpenAI (se usa para convertir el nuevo reporte a vector y hacer RAG)<br>\n",
    "Chat - OpenAI o LLama (genera el etiquetado una vez hecho el RAG)<br>\n",
    "BD vectorial - Qdrant<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setear pipeline y prompt\n",
    "# from haystack.components.validators import JsonSchemaValidator\n",
    "\n",
    "NUM_EXAMPLES = 0\n",
    "\n",
    "text_embedder = OpenAITextEmbedder(model=\"text-embedding-3-large\")\n",
    "retriever = QdrantEmbeddingRetriever(document_store ,top_k=NUM_EXAMPLES)\n",
    "if CHAT_MODEL == \"gpt-4o-mini\" or CHAT_MODEL == \"gpt-4o\":\n",
    "    if NUM_EXAMPLES > 0:\n",
    "        template = open(data_path + \"RE/prompt_gpt.txt\", \"r\").read()\n",
    "    elif NUM_EXAMPLES == 0:\n",
    "        template = open(data_path + \"RE/prompt_gpt_zero-shot.txt\", \"r\").read()\n",
    "        retriever = QdrantEmbeddingRetriever(document_store ,top_k=1)\n",
    "    llm = OpenAIGenerator(model=CHAT_MODEL)\n",
    "elif CHAT_MODEL == \"llama3.1\":\n",
    "    template = open(data_path + \"RE/prompt_ollama.txt\", \"r\").read()\n",
    "    llm = OllamaGenerator(model=CHAT_MODEL, url=\"http://localhost:11434/api/generate\")\n",
    "else:\n",
    "    raise ValueError(\"CHAT_MODEL debe ser 'gpt-4o', 'gpt-4o-mini' o 'llama3.1'\")\n",
    "\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "rag_pipeline.add_component(\"tracer\", LangfuseConnector(\"Mammography Few-Shot RAG Rel Ext \"+ CHAT_MODEL))\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"llm\", llm)\n",
    "# rag_pipeline.add_component(\"schema_validator\", JsonSchemaValidator())\n",
    "\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "# rag_pipeline.connect(\"llm.response\", \"schema_validator.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts a la API del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir dataset a enviar mediante prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_file_path = data_path + \"test.jsonl\"\n",
    "\n",
    "# load validation data\n",
    "\n",
    "prompt_reports = pd.read_json(reports_file_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop de envío"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "responses = []\n",
    "# Setear número de reportes a enviar a la API, es un prompt por reporte\n",
    "NUM_REPORTS = 69 # 69 es el total del conjunto de test\n",
    "\n",
    "# \n",
    "processed_ids_path = \"processed_ids.txt\"\n",
    "if os.path.exists(processed_ids_path):\n",
    "    with open(processed_ids_path, \"r\") as f:\n",
    "        processed_ids_list = f.read().splitlines()\n",
    "else:\n",
    "    processed_ids_list = []\n",
    "\n",
    "for i, informe in enumerate(prompt_reports[\"text\"][:NUM_REPORTS]):\n",
    "    entities, relations = convert_ids(prompt_reports[\"entities\"][i], prompt_reports[\"relations\"][i])\n",
    "    # para cada entidad, se agrega el texto del span en la llave \"span_text\"\n",
    "    for entity in entities:\n",
    "        entity[\"span_text\"] = informe[entity[\"start_offset\"]:entity[\"end_offset\"]]\n",
    "    query = informe + \"\\nEntidades:\\n\" + str(json.dumps(entities, indent=4, ensure_ascii=False))\n",
    "    report_id = prompt_reports[\"id\"][i]\n",
    "    # si el id del reporte ya está en la lista de ids procesados, no se envía a la API\n",
    "    if report_id in processed_ids_list:\n",
    "        print(\"Reporte \", i+1, \" de \", NUM_REPORTS, \" ya procesado\")\n",
    "        continue\n",
    "    result = rag_pipeline.run(data={\"prompt_builder\": {\"query\":query}, \"text_embedder\": {\"text\": query}})\n",
    "    # if the string contains ```json and ``` remove them\n",
    "    result_str = result[\"llm\"][\"replies\"][0]\n",
    "    result_str = result_str.replace(\"```json\", \"\")\n",
    "    result_str = result_str.replace(\"```\", \"\")\n",
    "    # convert result from string to a list of dictionaries\n",
    "    try:\n",
    "        result_json = eval(result_str)\n",
    "        result_json = {\"id\": report_id, \"relations\": result_json}\n",
    "        responses.append(result_json)\n",
    "        print(\"Procesado reporte \", i+1, \" de \", NUM_REPORTS)\n",
    "        processed_ids_list.append(report_id)\n",
    "    except:\n",
    "        print(\"Error en el reporte \", i+1, \" de \", NUM_REPORTS)\n",
    "    print(\"id: \", report_id)\n",
    "    # print(informe)\n",
    "    # print(json.dumps(result_json, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Guardar el listado de ids procesados en un archivo txt en la carpeta actual\n",
    "with open(processed_ids_path, \"w\") as f:\n",
    "    for item in processed_ids_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "print(\"Número de reportes enviados a la API: \", len(responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Arreglar spans y guardar respuestas en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar resultados en un archivo jsonl en la carpeta actual, append mode\n",
    "output_file_path = \"output.jsonl\"\n",
    "\n",
    "with open(output_file_path, \"a\") as f:\n",
    "    for item in responses:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_relations(relation1, relation2):\n",
    "    if relation1[\"type\"] == relation2[\"type\"] and relation1[\"from_id\"] == relation2[\"from_id\"] and relation1[\"to_id\"] == relation2[\"to_id\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "    \n",
    "# contar tp, fp, fn para cada clase para un reporte\n",
    "def calculate_tp_fp_fn_report(gold_relations, generated_relations, classes):\n",
    "    num_classes = len(classes)\n",
    "    tp = [0]*num_classes\n",
    "    fp = [0]*num_classes\n",
    "    fn = [0]*num_classes\n",
    "\n",
    "    for gold_relation in gold_relations:\n",
    "        found = False\n",
    "        for generated_relation in generated_relations:\n",
    "            if compare_relations(gold_relation, generated_relation):\n",
    "                tp[classes.index(gold_relation[\"type\"])] += 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            fn[classes.index(gold_relation[\"type\"])] += 1\n",
    "\n",
    "    for generated_relation in generated_relations:\n",
    "        found = False\n",
    "        for gold_relation in gold_relations:\n",
    "            if compare_relations(gold_relation, generated_relation):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            if generated_relation[\"type\"] in classes:\n",
    "                fp[classes.index(generated_relation[\"type\"])] += 1\n",
    "\n",
    "    return tp, fp, fn\n",
    "\n",
    "# calcular precision, recall y f1 dados tp, fp y fn\n",
    "def calculate_metrics(tp, fp, fn):\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    precision = tp/(tp+fp) if tp+fp > 0 else 0\n",
    "    recall = tp/(tp+fn) if tp+fn > 0 else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if precision+recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejecución (cálculo de métricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer entidades desde el archivo jsonl output.jsonl\n",
    "fixed_responses = []\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        fixed_responses.append(json.loads(line))\n",
    "\n",
    "print(\"Número de informes con relaciones extraídas: \", len(fixed_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or open csv file to save results\n",
    "results_file_path = data_path + \"RE/results.csv\"\n",
    "if not os.path.exists(results_file_path):\n",
    "    results_df = pd.DataFrame(columns=[\"chat_model\",\"fecha/hora\", \"num_reports\", \"macro_f1\", \"micro_f1\", \"ubicar_f1\", \"describir_f1\"])\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "else:\n",
    "    results_df = pd.read_csv(results_file_path)\n",
    "\n",
    "# calcular tp, fp, fn para cada clase\n",
    "classes = [\"ubicar\", \"describir\"]\n",
    "gold_relations_counts = {}\n",
    "tp_total = [0]*len(classes)\n",
    "fp_total = [0]*len(classes)\n",
    "fn_total = [0]*len(classes)\n",
    "\n",
    "        \n",
    "# por cada informe, llamar a la función calculate_tp_fp_fn_report. Sumar los resultados de cada informe para calcular tp, fp, fn totales.\n",
    "for i, informe in enumerate(prompt_reports[\"text\"][:NUM_REPORTS]):\n",
    "    gold_entities, gold_relations = convert_ids(prompt_reports[\"entities\"][i], prompt_reports[\"relations\"][i])\n",
    "    # contar cantidad de entidades de cada clase\n",
    "    for relation in gold_relations:\n",
    "        if relation[\"type\"] in gold_relations_counts:\n",
    "            gold_relations_counts[relation[\"type\"]] += 1\n",
    "        else:\n",
    "            gold_relations_counts[relation[\"type\"]] = 1\n",
    "\n",
    "    # buscar entidades corregidas del informe, buscando por id en la lista fixed_responses\n",
    "    generated_relations = []\n",
    "    for response in fixed_responses:\n",
    "        if response[\"id\"] == prompt_reports[\"id\"][i]:\n",
    "            generated_relations = response[\"relations\"]\n",
    "    if len(generated_relations) == 0:\n",
    "        print(\"No se encontraron entidades corregidas para el informe \", i)\n",
    "        continue\n",
    "    tp, fp, fn = calculate_tp_fp_fn_report(gold_relations, generated_relations, classes)\n",
    "    tp_total = [sum(x) for x in zip(tp_total, tp)]\n",
    "    fp_total = [sum(x) for x in zip(fp_total, fp)]\n",
    "    fn_total = [sum(x) for x in zip(fn_total, fn)]\n",
    "\n",
    "# calcular precision, recall y f1 para cada clase\n",
    "metrics_per_class = []\n",
    "for i, class_name in enumerate(classes):\n",
    "    print(\"Clase: \", class_name)\n",
    "    metrics = calculate_metrics(tp_total[i], fp_total[i], fn_total[i])\n",
    "    print(\"Precision: \", metrics[0])\n",
    "    print(\"Recall: \", metrics[1])\n",
    "    print(\"F1: \", metrics[2])\n",
    "    metrics_per_class.append(metrics)\n",
    "    # mostrar cantidad de entidades de la clase\n",
    "    if class_name in gold_relations_counts:\n",
    "        print(\"Cantidad de entidades de la clase: \", gold_relations_counts[class_name])\n",
    "\n",
    "# macro-average\n",
    "macro_precision = 0\n",
    "macro_recall = 0\n",
    "macro_f1 = 0\n",
    "\n",
    "for i, class_name in enumerate(classes):\n",
    "    macro_precision += metrics_per_class[i][0]/len(classes)\n",
    "    macro_recall += metrics_per_class[i][1]/len(classes)\n",
    "    macro_f1 += metrics_per_class[i][2]/len(classes)\n",
    "\n",
    "print(\"Macro-average\")\n",
    "print(\"Precision: \", macro_precision)\n",
    "print(\"Recall: \", macro_recall)\n",
    "print(\"F1: \", macro_f1)\n",
    "\n",
    "# micro-average\n",
    "\n",
    "micro_precision, micro_recall, micro_f1 = calculate_metrics(sum(tp_total), sum(fp_total), sum(fn_total))\n",
    "\n",
    "print(\"Micro-average\")\n",
    "print(\"Precision: \", micro_precision)\n",
    "print(\"Recall: \", micro_recall)\n",
    "print(\"F1: \", micro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar resultados en csv\n",
    "import datetime\n",
    "results_df = pd.concat([results_df, pd.DataFrame([[CHAT_MODEL,datetime.datetime.now(), NUM_REPORTS, macro_f1, micro_f1, metrics_per_class[0][2], metrics_per_class[1][2]]], columns=results_df.columns)], ignore_index=True)\n",
    "\n",
    "results_df.to_csv(results_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
